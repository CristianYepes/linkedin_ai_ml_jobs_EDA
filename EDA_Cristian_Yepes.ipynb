{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uKLjILGZc-zd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from textwrap import wrap\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (8,5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n"
      ],
      "metadata": {
        "id": "sllvbeOuePgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"kanchana1990/ai-and-ml-job-listings-usa\")\n",
        "\n",
        "print(\"Ruta local del dataset:\", path)\n"
      ],
      "metadata": {
        "id": "65Z3yzcreXop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1. Verificar que el dataset es tabular"
      ],
      "metadata": {
        "id": "2hNo_6MvxYCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path(path)\n",
        "print(\"Ruta local del dataset:\", path)\n",
        "\n",
        "all_files = [p for p in path.rglob(\"*\") if p.is_file()]\n",
        "print(f\"Archivos encontrados ({len(all_files)}):\")\n",
        "for p in all_files[:15]:\n",
        "    print(\" -\", p.name)\n",
        "\n",
        "img_exts = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".tiff\", \".webp\"}\n",
        "tabular_exts = {\".csv\", \".tsv\", \".xlsx\", \".xls\", \".parquet\"}\n",
        "\n",
        "has_images = any(p.suffix.lower() in img_exts for p in all_files)\n",
        "tabular_files = [p for p in all_files if p.suffix.lower() in tabular_exts]\n",
        "\n",
        "if has_images and not tabular_files:\n",
        "    raise ValueError(\"El dataset parece tener imágenes y no archivos tabulares. Escoge otro dataset solo de datos.\")\n",
        "elif not tabular_files:\n",
        "    raise ValueError(\"No se detectaron archivos tabulares (.csv/.tsv/.xlsx/.xls/.parquet). Revisa el contenido del dataset.\")\n",
        "else:\n",
        "    print(\"\\nOK: Se detectaron archivos tabulares:\")\n",
        "    for p in tabular_files:\n",
        "        print(\" -\", p.name)\n",
        "\n",
        "def file_priority(p: Path) -> int:\n",
        "    ext = p.suffix.lower()\n",
        "    if ext == \".parquet\": return 0\n",
        "    if ext in [\".csv\", \".tsv\"]: return 1\n",
        "    if ext in [\".xlsx\", \".xls\"]: return 2\n",
        "    return 9\n",
        "\n",
        "tabular_files_sorted = sorted(tabular_files, key=lambda p: (file_priority(p), -p.stat().st_size))\n",
        "DATA_FILE = tabular_files_sorted[0]\n",
        "print(f\"\\nArchivo principal elegido: {DATA_FILE.name} (tamaño ~ {DATA_FILE.stat().st_size/1e6:.2f} MB)\")"
      ],
      "metadata": {
        "id": "MXDRMj73xLgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2. Cargar el CSV"
      ],
      "metadata": {
        "id": "5dQpdcJGyVqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_FILE = next(p for p in Path(path).rglob(\"ai_ml_jobs_linkedin.csv\"))\n",
        "df = pd.read_csv(DATA_FILE, low_memory=False)\n",
        "print(f\"Cargado: {df.shape[0]:,} filas x {df.shape[1]:,} columnas\")\n",
        "display(df.head(5))"
      ],
      "metadata": {
        "id": "LF7NWDS8yPd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3. Chequeos básicos (nulos, duplicados, cardinalidad)"
      ],
      "metadata": {
        "id": "IEn-MiIezBZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTipos originales:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n% de nulos por columna (Top 20):\")\n",
        "nulls = df.isna().mean().sort_values(ascending=False)\n",
        "display(nulls.head(20).to_frame(\"% nulos\"))\n",
        "\n",
        "dup_count = df.duplicated().sum()\n",
        "print(f\"\\nDuplicados exactos: {dup_count}\")\n",
        "\n",
        "card = df.nunique(dropna=True).sort_values(ascending=False)\n",
        "print(\"\\nCardinalidad por columna (Top 20):\")\n",
        "display(card.head(20).to_frame(\"nunique\"))"
      ],
      "metadata": {
        "id": "a9X_XcYazETK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4. Tipificar columnas (num/cat/fecha)"
      ],
      "metadata": {
        "id": "3XWQg3CLzdjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"df_clean\" not in locals():\n",
        "    df_clean = df.copy()\n",
        "\n",
        "for col in df_clean.select_dtypes(include=[\"object\"]).columns:\n",
        "    df_clean[col] = df_clean[col].astype(str).str.strip().replace({\"nan\": np.nan})\n",
        "\n",
        "if \"publishedAt\" in df_clean.columns:\n",
        "    df_clean[\"publishedAt\"] = pd.to_datetime(df_clean[\"publishedAt\"], errors=\"coerce\", utc=True).dt.tz_localize(None)\n",
        "\n",
        "def parse_applicants(s):\n",
        "    if pd.isna(s):\n",
        "        return np.nan\n",
        "    nums = re.findall(r\"\\d+\", str(s))\n",
        "    if not nums:\n",
        "        return np.nan\n",
        "    return float(max(int(n) for n in nums))\n",
        "\n",
        "if \"applicationsCount_num\" not in df_clean.columns:\n",
        "    if \"applicationsCount\" in df_clean.columns:\n",
        "        df_clean[\"applicationsCount_num\"] = df_clean[\"applicationsCount\"].apply(parse_applicants)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "if \"applicationsCount_num\" in df_clean.columns and \"is_top200\" not in df_clean.columns:\n",
        "    df_clean[\"is_top200\"] = (df_clean[\"applicationsCount_num\"] >= 200).astype(\"Int64\")\n",
        "\n",
        "if \"applicationsCount_num\" in df_clean.columns and \"publishedAt\" in df_clean.columns and \"apps_per_day\" not in df_clean.columns:\n",
        "    today = pd.Timestamp.today().normalize()\n",
        "    days = (today - df_clean[\"publishedAt\"]).dt.days\n",
        "    days = days.clip(lower=1)\n",
        "    df_clean[\"apps_per_day\"] = df_clean[\"applicationsCount_num\"] / days\n",
        "    df_clean.loc[df_clean[\"apps_per_day\"] > 1000, \"apps_per_day\"] = np.nan\n",
        "\n",
        "if \"title\" in df_clean.columns:\n",
        "    df_clean[\"title_len\"] = df_clean[\"title\"].astype(str).str.len()\n",
        "if \"description\" in df_clean.columns:\n",
        "    df_clean[\"desc_len\"] = df_clean[\"description\"].astype(str).str.len()\n",
        "if \"location\" in df_clean.columns and \"state\" not in df_clean.columns:\n",
        "    df_clean[\"state\"] = (\n",
        "        df_clean[\"location\"].astype(str).str.extract(r\",\\s*([A-Z]{2})\", expand=False)\n",
        "    )\n",
        "if \"sector\" in df_clean.columns and \"sector_main\" not in df_clean.columns:\n",
        "    df_clean[\"sector_main\"] = df_clean[\"sector\"].astype(str).str.split(\",\").str[0].str.strip()\n",
        "if \"location\" in df_clean.columns and \"location_clean\" not in df_clean.columns:\n",
        "    bad_locs = {\"United States\", \"New York, United States\"}\n",
        "    df_clean[\"location_clean\"] = df_clean[\"location\"].where(~df_clean[\"location\"].isin(bad_locs), np.nan)\n",
        "\n",
        "for col in df_clean.select_dtypes(include=[\"object\"]).columns:\n",
        "    nunq = df_clean[col].nunique(dropna=True)\n",
        "    if 2 <= nunq <= 50:\n",
        "        df_clean[col] = df_clean[col].astype(\"category\")\n",
        "\n",
        "num_base = [c for c in [\"applicationsCount_num\",\"apps_per_day\",\"title_len\",\"desc_len\"] if c in df_clean.columns]\n",
        "for c in num_base:\n",
        "    s = df_clean[c]\n",
        "    if s.notna().any() and not (s.dropna() < 0).any():\n",
        "        df_clean[f\"log1p_{c}\"] = np.log1p(s)\n",
        "\n",
        "for c in [\"title_len\",\"desc_len\"]:\n",
        "    if c in df_clean.columns and f\"{c}_win\" not in df_clean.columns:\n",
        "        hi = df_clean[c].quantile(0.99)\n",
        "        df_clean[f\"{c}_win\"] = np.minimum(df_clean[c], hi)\n",
        "\n",
        "created = [c for c in [\"applicationsCount_num\",\"is_top200\",\"apps_per_day\",\n",
        "                       \"title_len\",\"desc_len\",\"title_len_win\",\"desc_len_win\",\n",
        "                       \"log1p_applicationsCount_num\",\"log1p_apps_per_day\",\n",
        "                       \"log1p_title_len\",\"log1p_desc_len\",\n",
        "                       \"state\",\"sector_main\",\"location_clean\"]\n",
        "           if c in df_clean.columns]\n",
        "print(\"Features creadas/actualizadas:\", created)\n",
        "print(\"Tipos (resumen):\")\n",
        "print(df_clean[created].dtypes if created else \"—\")"
      ],
      "metadata": {
        "id": "UVaPuEYSEa-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PASO 5. Distribuciones"
      ],
      "metadata": {
        "id": "TGiQJ3g8047I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = [\"applicationsCount_num\", \"title_len\", \"desc_len\"]\n",
        "num_cols = [c for c in num_cols if c in df_clean.columns]\n",
        "\n",
        "cat_cols = []\n",
        "for c in [\"contractType\",\"experienceLevel\",\"workType\",\"state\",\"sector\"]:\n",
        "    if c in df_clean.columns:\n",
        "        cat_cols.append(c)\n",
        "\n",
        "print(f\"Numéricas: {num_cols}\")\n",
        "print(f\"Categóricas: {cat_cols}\")\n",
        "\n",
        "for col in num_cols:\n",
        "    ax = df_clean[col].plot(kind=\"hist\", bins=40, title=f\"Histograma: {col}\")\n",
        "    ax.set_xlabel(col); ax.set_ylabel(\"Frecuencia\")\n",
        "    plt.show()\n",
        "\n",
        "for col in cat_cols:\n",
        "    vc = df_clean[col].value_counts(dropna=False).head(20)\n",
        "    ax = vc.plot(kind=\"bar\", title=f\"{col} (Top 20)\")\n",
        "    ax.set_xlabel(col); ax.set_ylabel(\"Frecuencia\")\n",
        "    plt.show()\n",
        "\n",
        "if \"publishedAt\" in df_clean.columns:\n",
        "    s = (\n",
        "        df_clean.set_index(\"publishedAt\")\n",
        "                .sort_index()\n",
        "                .resample(\"W\")\n",
        "                .size()\n",
        "    )\n",
        "    ax = s.plot(kind=\"line\", title=\"Publicaciones por semana\")\n",
        "    ax.set_xlabel(\"Fecha\"); ax.set_ylabel(\"Conteo\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "H-hGyW9K065Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PASO 6. Univariable"
      ],
      "metadata": {
        "id": "f8i186_715Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if num_cols:\n",
        "    desc_num = df_clean[num_cols].describe().T\n",
        "    desc_num[\"missing_%\"] = 100*(1 - df_clean[num_cols].notna().mean())\n",
        "    desc_num[\"skew\"] = df_clean[num_cols].skew(numeric_only=True)\n",
        "    desc_num[\"kurt\"] = df_clean[num_cols].kurt(numeric_only=True)\n",
        "    print(\"\\nEstadísticos numéricos:\")\n",
        "    display(desc_num)\n",
        "\n",
        "for col in cat_cols:\n",
        "    print(f\"\\nTop categorías: {col}\")\n",
        "    display(df_clean[col].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "4w7027I318b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PASO 7. Multivariable"
      ],
      "metadata": {
        "id": "llynlr4D2gWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"applicationsCount_num\"\n",
        "TOP_K = 20\n",
        "MIN_COUNT = 10\n",
        "WRAP_WIDTH = 26\n",
        "MAX_LINES = 2\n",
        "\n",
        "def wrap_or_trim(labels, width=26, max_lines=2):\n",
        "    out = []\n",
        "    for lab in labels:\n",
        "        s = str(lab)\n",
        "        if \",\" in s and len(s) > width:\n",
        "            parts = [p.strip() for p in s.split(\",\")]\n",
        "            s = \", \".join(parts[:2])\n",
        "        w = wrap(s, width=width)\n",
        "        if len(w) > max_lines:\n",
        "            w = w[:max_lines-1] + [w[max_lines-1][:max(0, width-3)] + \"...\"]\n",
        "        out.append(\"\\n\".join(w))\n",
        "    return out\n",
        "\n",
        "if \"sector\" in df_clean.columns and \"sector_main\" not in df_clean.columns:\n",
        "    df_clean[\"sector_main\"] = df_clean[\"sector\"].astype(str).str.split(\",\").str[0].str.strip()\n",
        "\n",
        "bad_locs = {\"United States\", \"New York, United States\"}\n",
        "if \"location\" in df_clean.columns and \"location_clean\" not in df_clean.columns:\n",
        "    df_clean[\"location_clean\"] = df_clean[\"location\"].where(~df_clean[\"location\"].isin(bad_locs), np.nan)\n",
        "\n",
        "cat_cols = [c for c in [\n",
        "    \"contractType\",\"experienceLevel\",\"workType\",\n",
        "    \"state\",\"sector_main\",\"sector\",\n",
        "    \"companyName\",\"title\",\"location_clean\",\"location\"\n",
        "] if c in df_clean.columns and df_clean[c].dtype.name in [\"object\",\"category\"]]\n",
        "\n",
        "def barras_medianas_sin_error(col, top_k=TOP_K, min_count=MIN_COUNT):\n",
        "    agg = (\n",
        "        df_clean.groupby(col, observed=True)[TARGET]\n",
        "        .agg(n=\"count\", mediana=\"median\")\n",
        "        .query(\"n >= @min_count\")\n",
        "        .sort_values(\"mediana\", ascending=False)\n",
        "        .head(top_k)\n",
        "    )\n",
        "    if agg.empty: return\n",
        "\n",
        "    x = agg[\"mediana\"].values\n",
        "    ylabels = wrap_or_trim(agg.index.tolist(), WRAP_WIDTH, MAX_LINES)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(range(len(x)), x)\n",
        "    plt.yticks(range(len(x)), ylabels)\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    max_x = x.max()\n",
        "    plt.xlim(0, max_x * 1.1)\n",
        "    plt.xlabel(f\"Mediana de {TARGET}\")\n",
        "    plt.title(f\"Mediana de {TARGET} por {col} (Top {len(x)}, min {min_count} filas)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    print(f\"\\n{col} — Top {len(agg)} por mediana (min {min_count} filas)\")\n",
        "    display(agg)\n",
        "\n",
        "def barras_prop_tope_sin_error(col, threshold=200, top_k=TOP_K, min_count=MIN_COUNT):\n",
        "    df_aux = df_clean[[col, TARGET]].dropna(subset=[col])\n",
        "    if df_aux.empty: return\n",
        "    df_aux = df_aux.assign(al_tope = df_aux[TARGET] >= threshold)\n",
        "    agg = (\n",
        "        df_aux.groupby(col, observed=True)\n",
        "        .agg(n=(TARGET, \"count\"), prop_top=(\"al_tope\", \"mean\"))\n",
        "        .query(\"n >= @min_count\")\n",
        "        .sort_values(\"prop_top\", ascending=False)\n",
        "        .head(top_k)\n",
        "    )\n",
        "    if agg.empty: return\n",
        "\n",
        "    x = (agg[\"prop_top\"] * 100).values\n",
        "    ylabels = wrap_or_trim(agg.index.tolist(), WRAP_WIDTH, MAX_LINES)\n",
        "\n",
        "    plt.figure(figsize=(12, 5.5))\n",
        "    plt.barh(range(len(x)), x)\n",
        "    plt.yticks(range(len(x)), ylabels)\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    max_x = x.max()\n",
        "    pad = max(1e-9, max_x * 0.02)\n",
        "    plt.xlim(0, max_x * 1.1)\n",
        "    for i, v in enumerate(x):\n",
        "        plt.text(v + pad, i, f\"{v:.0f}%\", va=\"center\", ha=\"left\", fontsize=10)\n",
        "\n",
        "    plt.xlabel(f\"% de ofertas con ≥{threshold} aplicaciones\")\n",
        "    plt.title(f\"% al tope por {col} (Top {len(x)}, min {min_count} filas)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    print(f\"\\n{col} — % al tope (≥{threshold})\")\n",
        "    display(agg)\n",
        "\n",
        "for col in cat_cols:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Análisis categórica: {col}\")\n",
        "    barras_medianas_sin_error(col)\n",
        "    barras_prop_tope_sin_error(col)"
      ],
      "metadata": {
        "id": "tEftX_Uf2iRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if \"apps_per_day\" not in df_clean.columns and \"publishedAt\" in df_clean.columns:\n",
        "    today = pd.Timestamp.today().normalize()\n",
        "    days = (today - df_clean[\"publishedAt\"]).dt.days\n",
        "    days = days.clip(lower=1)\n",
        "    df_clean[\"apps_per_day\"] = df_clean[\"applicationsCount_num\"] / days\n",
        "    df_clean.loc[df_clean[\"apps_per_day\"] > 1000, \"apps_per_day\"] = np.nan\n"
      ],
      "metadata": {
        "id": "pX4lIDni-IlD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PASO 8. Outliers"
      ],
      "metadata": {
        "id": "Fl-nZFZL-ZgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_cols = [c for c in [\"applicationsCount_num\", \"title_len\", \"desc_len\", \"apps_per_day\"]\n",
        "            if c in df_clean.columns]\n",
        "\n",
        "P_LOW, P_HIGH = 0.01, 0.99\n",
        "BINS = 40\n",
        "\n",
        "def iqr_bounds(s, k=1.5):\n",
        "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    return q1 - k*iqr, q3 + k*iqr\n",
        "\n",
        "def _clip_for_plot(s: pd.Series, low=P_LOW, high=P_HIGH):\n",
        "    s = s.dropna()\n",
        "    if s.empty:\n",
        "        return s, None, None\n",
        "    lo, hi = s.quantile(low), s.quantile(high)\n",
        "    return s.clip(lo, hi), lo, hi\n",
        "\n",
        "def hist_safe(s: pd.Series, name: str, bins=BINS):\n",
        "    s = s.dropna()\n",
        "    if s.empty:\n",
        "        return\n",
        "    s_clip, lo, hi = _clip_for_plot(s)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(s_clip.values, bins=bins)\n",
        "    plt.title(f\"Histograma (cap {int(P_LOW*100)}–{int(P_HIGH*100)}%): {name}\")\n",
        "    plt.xlabel(name); plt.ylabel(\"Frecuencia\")\n",
        "\n",
        "    if s.skew() > 1:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.hist(np.log1p(s.values), bins=bins)\n",
        "        plt.title(f\"Histograma: log1p({name})\")\n",
        "        plt.xlabel(f\"log1p({name})\"); plt.ylabel(\"Frecuencia\")\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "def box_safe(s: pd.Series, name: str):\n",
        "    s = s.dropna()\n",
        "    if s.empty:\n",
        "        return\n",
        "    s_clip, lo, hi = _clip_for_plot(s)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.boxplot(s_clip.values, showfliers=False)\n",
        "    plt.title(f\"Boxplot (sin fliers, cap {int(P_LOW*100)}–{int(P_HIGH*100)}%): {name}\")\n",
        "    plt.ylabel(name)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "rows = []\n",
        "for col in num_cols:\n",
        "    s = df_clean[col].dropna()\n",
        "    if s.empty:\n",
        "        continue\n",
        "    low, high = iqr_bounds(s, 1.5)\n",
        "    iqr_flags = (s < low) | (s > high)\n",
        "    z = (s - s.mean())/s.std(ddof=0)\n",
        "    z_flags = z.abs() > 3\n",
        "    rows.append({\n",
        "        \"columna\": col, \"n\": int(s.size),\n",
        "        \"%_outliers_IQR\": round(iqr_flags.mean()*100, 2),\n",
        "        \"%_outliers_Z>3\": round(z_flags.mean()*100, 2),\n",
        "        \"lim_inf_IQR\": low, \"p25\": s.quantile(.25), \"mediana\": s.median(),\n",
        "        \"p75\": s.quantile(.75), \"lim_sup_IQR\": high,\n",
        "        \"min\": s.min(), \"max\": s.max(), \"skew\": round(s.skew(), 3)\n",
        "    })\n",
        "\n",
        "out_df = pd.DataFrame(rows).sort_values(\"%_outliers_IQR\", ascending=False)\n",
        "print(\"\\nResumen de outliers (IQR y Z>3):\")\n",
        "display(out_df)\n",
        "\n",
        "for col in num_cols:\n",
        "    s = df_clean[col]\n",
        "    box_safe(s, col)\n",
        "    hist_safe(s, col)"
      ],
      "metadata": {
        "id": "1BWtLSix-edR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  PASO 9. Correlaciones"
      ],
      "metadata": {
        "id": "C4Q6QtrYAXK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAW_COLS = [c for c in [\"applicationsCount_num\",\"apps_per_day\",\"title_len\",\"desc_len\"] if c in df_clean.columns]\n",
        "USE_LOG_ON_THE_FLY = True\n",
        "SCATTER_SAMPLE_MAX = 10000\n",
        "P_LOW, P_HIGH = 0.01, 0.99\n",
        "POINT_SIZE = 8\n",
        "ALPHA = 0.3\n",
        "\n",
        "def build_log_view(df, cols):\n",
        "    \"\"\"Devuelve un DataFrame con log1p de cols si todas existen y son >=0 (al menos no-negativas).\n",
        "       No modifica df_clean: es una vista temporal.\"\"\"\n",
        "    if not cols:\n",
        "        return None, []\n",
        "    ok = []\n",
        "    for c in cols:\n",
        "        s = df[c].dropna()\n",
        "        if s.empty:\n",
        "            continue\n",
        "        if (s < 0).any():\n",
        "            continue\n",
        "        ok.append(c)\n",
        "    if not ok:\n",
        "        return None, []\n",
        "    df_log = pd.DataFrame({f\"log1p_{c}\": np.log1p(df[c]) for c in ok})\n",
        "    return df_log, [f\"log1p_{c}\" for c in ok]\n",
        "\n",
        "def corr_and_top_pairs(df_num, name):\n",
        "    cols = df_num.columns.tolist()\n",
        "    if len(cols) < 2:\n",
        "        print(f\"\\n[{name}] No hay suficientes columnas para correlaciones.\")\n",
        "        return None, None\n",
        "    print(f\"\\n=== Correlaciones ({name}) ===\")\n",
        "    corr_s = df_num.corr(method=\"spearman\")\n",
        "    corr_k = df_num.corr(method=\"kendall\")\n",
        "    print(\"Spearman:\"); display(corr_s)\n",
        "    print(\"Kendall:\");  display(corr_k)\n",
        "\n",
        "    pair_scores = []\n",
        "    for a, b in combinations(cols, 2):\n",
        "        val = abs(corr_s.loc[a, b])\n",
        "        if pd.notna(val):\n",
        "            pair_scores.append(((a, b), val))\n",
        "    pair_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_pairs = pair_scores[:10]\n",
        "    if top_pairs:\n",
        "        print(\"Top pares (|Spearman|):\")\n",
        "        display(pd.Series({f\"{a} ~ {b}\": v for (a, b), v in top_pairs}))\n",
        "    else:\n",
        "        print(\"Sin pares válidos.\")\n",
        "    return corr_s, top_pairs\n",
        "\n",
        "def scatter_robusto(df, x, y, title=None):\n",
        "    s = df[[x, y]].dropna()\n",
        "    if s.empty:\n",
        "        return\n",
        "    if len(s) > SCATTER_SAMPLE_MAX:\n",
        "        s = s.sample(SCATTER_SAMPLE_MAX, random_state=42)\n",
        "\n",
        "    xlo, xhi = s[x].quantile(P_LOW), s[x].quantile(P_HIGH)\n",
        "    ylo, yhi = s[y].quantile(P_LOW), s[y].quantile(P_HIGH)\n",
        "    sx = s[x].clip(xlo, xhi)\n",
        "    sy = s[y].clip(ylo, yhi)\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(sx, sy, s=POINT_SIZE, alpha=ALPHA)\n",
        "    plt.xlabel(x); plt.ylabel(y)\n",
        "    plt.title(title if title else f\"{x} vs {y} (cap {int(P_LOW*100)}–{int(P_HIGH*100)}%)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "if RAW_COLS:\n",
        "    df_raw = df_clean[RAW_COLS]\n",
        "    _, top_pairs_raw = corr_and_top_pairs(df_raw, \"RAW\")\n",
        "else:\n",
        "    top_pairs_raw = None\n",
        "    print(\"No hay columnas numéricas RAW.\")\n",
        "\n",
        "top_pairs_log = None\n",
        "if USE_LOG_ON_THE_FLY:\n",
        "    df_log, LOG_COLS = build_log_view(df_clean, RAW_COLS)\n",
        "    if df_log is not None and len(LOG_COLS) >= 2:\n",
        "        _, top_pairs_log = corr_and_top_pairs(df_log, \"LOG1P (al vuelo)\")\n",
        "    else:\n",
        "        print(\"\\n[LOG1P] No se generaron suficientes columnas válidas para correlaciones (valores negativos o faltantes).\")\n",
        "\n",
        "pairs_to_plot = []\n",
        "if top_pairs_log:\n",
        "    pairs_to_plot = top_pairs_log[:5]\n",
        "elif top_pairs_raw:\n",
        "    pairs_to_plot = top_pairs_raw[:5]\n",
        "if pairs_to_plot:\n",
        "    print(\"\\nScatter de los pares principales:\")\n",
        "    if top_pairs_log:\n",
        "        df_for_scatter = df_log\n",
        "    else:\n",
        "        df_for_scatter = df_raw\n",
        "    for (a, b), score in pairs_to_plot:\n",
        "        scatter_robusto(df_for_scatter, a, b, title=f\"{a} vs {b}  (|ρ|={score:.2f})\")\n",
        "else:\n",
        "    print(\"\\nNo hay pares suficientes para graficar.\")\n"
      ],
      "metadata": {
        "id": "8gtQ2zzSAoFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}